{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38b9d96-59a8-4580-a3c0-8f86a8abbffb",
   "metadata": {},
   "source": [
    "### --> PIPELINE QUI CONSTRUIT F1 et F2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb137967-ed9f-46f6-a72d-6886fe9b6749",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c0e357-7196-4b77-9e1c-72bb39eeae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "from lxml import html\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.python.lib.io import file_io\n",
    "#jhvkfvhkqhndvv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0ceb9-71e7-4433-bd99-2fadcfb56740",
   "metadata": {},
   "source": [
    "### DEFINITION FONCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5991c0-3332-44da-82d0-2ecfadbc3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nom_normalizer(equipe_a_normaliser):\n",
    "    teams = {\n",
    "        'OLYMPIQUE LYONNAIS': ['OLYMPIQUE LYONNAIS', 'LYON', 'OL', 'O.L.'], \n",
    "        'AC AJACCIO': ['AC AJACCIO', 'AJACCIO', 'ACA', 'A.C.A', 'ATHLETIC CLUB AJACCIEN','A-C AJACCIO','A.C AJACCIO'],\n",
    "        'RC STRASBOURG ALSACE' : ['STRASBOURG','RACING CLUB DE STRASBOURG ALSACE', 'RC STRASBOURG','R.C. STRASBOURG','R.C.S.','RCS', 'R.C.S.A.', 'RC STRASBOURG ALSACE', 'R.C. STRASBOURG ALSACE'], \n",
    "        'AS MONACO': ['AS MONACO', 'MONACO', 'ASSOCIATION SPORTIVE DE MONACO FOOTBALL CLUB', 'AS MONACO FC', 'A.S MONACO F.C', 'ASM'], \n",
    "        'CLERMONT FOOT 63': ['CLERMONT FOOT 63', 'CLERMONT-FERRAND', 'CF63', 'C.F.63', 'C-F 63', 'CLERMONT FOOT'], \n",
    "        'PARIS SAINT GERMAIN': ['PARIS SAINT GERMAIN', 'PARIS', 'PARIS-SAINT-GERMAIN FOOTBALL CLUB', 'PSG', 'P.S.G.', 'PARIS SG', 'PARIS-SG', 'PARIS SAINT-GERMAIN', 'PARIS SAINT-GERMAIN FOOTBALL CLUB', 'PARIS SAINT-GERMAIN FC'], \n",
    "        'TOULOUSE FC' : ['TOULOUSE', 'TOULOUSE FOOTBALL CLUB', 'TFC', 'T.F.C.', 'TOULOUSE FC', 'TOULOUSE F.C.'], \n",
    "        'OGC NICE': ['NICE', \"OLYMPIQUE GYMNASTE CLUB NICE COTE D'AZUR\", 'OGC NICE', 'O.G.C. NICE', \"OGC NICE COTE D'AZUR\", \"O.G.C. NICE COTE D'AZUR\", 'OLYMPIQUE GYMNASTE CLUB NICE'], \n",
    "        'ANGERS SCO': ['ANGERS SCO', 'ANGERS', \"SCO D'ANGERS\", 'SCOA', \"ANGERS SPORTING CLUB DE L'OUEST\", 'SCO ANGERS', 'ANGERS-SCO', 'ANGERS S.C.O.', 'S.C.O. ANGERS'], \n",
    "        'FC NANTES' : ['FC NANTES', 'NANTES', 'FOOTBALL CLUB DE NANTES', 'F.C. NANTES', 'FCN', 'F.C.N.'], \n",
    "        'RC LENS': ['RC LENS', 'LENS', 'RACING CLUB DE LENS', 'RCL', 'R.C.L.', 'R.C. LENS'], \n",
    "        'STADE BRESTOIS': ['STADE BRESTOIS', 'BREST', 'STADE BRESTOIS 29', 'SB29', 'S.B.29', 'S.B. 29'], \n",
    "        'LOSC LILLE': ['LOSC LILLE', 'LILLE', 'LOSC', 'LILLE OLYMPIQUE SPORTING CLUB', 'L.O.S.C.', 'L.O.S.C. LILLE'], \n",
    "        'AJ AUXERRE': ['AJ AUXERRE', 'AUXERRE', 'AJA', 'ASSOCIATION DE LA JEUNESSE AUXERROISE', 'A.J.AUXERRE', 'A.J AUXERRE'], \n",
    "        'MONTPELLIER HÉRAULT SC': ['MONTPELLIER HÉRAULT SC','MONTPELLIER HÉRAULT', 'MONTPELLIER', 'MONTPELLIER-HERAULT SPORT CLUB', 'MONTPELLIER HERAULT SPORT CLUB', 'MONTPELLIER HSC', 'MHSC', 'MONTPELLIER-HERAULT SC', 'MONTPELLIER-HÉRAULT SC', 'MONTPELLIER-HERAULT S.C.', 'M.H.S.C.', 'MONTPELLIER HERAULT SC', 'MONTPELLIER HÉRAULT SC', 'MONTPELLIER HERAULT S.C.'], \n",
    "        'ESTAC TROYES': ['TROYES', 'ESPERANCE SPORTIVE TROYES AUBE CHAMPAGNE', 'ESTAC', 'E.S.T.A.C.', 'ESTAC TROYES', 'E.S.T.A.C. TROYES'], \n",
    "        'STADE RENNAIS FC': ['RENNES', 'STADE RENNAIS FOOTBALL CLUB', 'STADE RENNAIS', 'STADE RENNAIS FC', 'S.R.F.C.', 'STADE RENNAIS F.C.'], \n",
    "        'FC LORIENT': ['FC LORIENT', 'LORIENT', 'FOOTBALL CLUB LORIENT BRETAGNE SUD', 'FCL', 'F.C.L.', 'F.C. LORIENT', 'FC LORIENT BRETAGNE SUD', 'FOOTBALL CLUB LORIENTAIS'], \n",
    "        'OLYMPIQUE DE MARSEILLE': ['OLYMPIQUE DE MARSEILLE', 'MARSEILLE', 'OM', 'O.M.'],\n",
    "        'STADE DE REIMS': ['STADE DE REIMS', 'REIMS', 'STADE REIMS'],\n",
    "        'DIJON': ['DIJON', 'DIJON FCO', 'DFCO', \"DIJON FOOTBALL COTE-D'OR\", \"DIJON FOOTBALL CÔTE-D'OR\"],\n",
    "        'SAINT ETIENNE': ['ST ETIENNE', 'ST-ETIENNE', 'ST ÉTIENNE', 'ST-ÉTIENNE', 'SAINT ETIENNE', 'SAINT-ETIENNE', 'SAINT ÉTIENNE', 'SAINT-ÉTIENNE', 'ASSE','AS SAINT ETIENNE', 'AS SAINT-ETIENNE', 'AS SAINT-ÉTIENNE', 'AS SAINT-ÉTIENNE'],\n",
    "        'BORDEAUX': ['BORDEAUX', 'FOOTBALL CLUB DES GIRONDINS DE BORDEAUX', 'FC GIRONDINS DE BORDEAUX', 'GIRONDINS DE BORDEAUX', 'FCG BORDEAUX', 'FOOTBALL CLUB GIRONDINS DE BORDEAUX'],\n",
    "        'METZ': ['METZ', 'FC METZ', 'FOOTBALL CLUB DE METZ', 'FOOTBALL CLUB METZ']}\n",
    "    resultat = equipe_a_normaliser\n",
    "    for equipe_name in teams:\n",
    "        if equipe_a_normaliser in teams[equipe_name]:\n",
    "            return equipe_name\n",
    "    return resultat\n",
    "\n",
    "def merge_season(season, path):\n",
    "    directory = f\"./Data_final/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    merged =open(directory+f\"season_{season}_{season+1}.csv\",\"a\")    \n",
    "    merged.write('Journee,HomeTeam,AwayTeam,FTHG,FTAG,URL\\n')  \n",
    "    for num in range(1,39):\n",
    "        f = open(path+\"/season_\"+str(season)+\"_\"+str(season+1)+\"_journee_\"+str(num)+\".csv\")\n",
    "        next(f) # skip the header\n",
    "        for line in f:\n",
    "            merged.write(f'{num},'+line)\n",
    "        f.close() # not really needed\n",
    "    merged.close()\n",
    "\n",
    "def merge_ligue1(season1,season2):\n",
    "    merged_F =open(\"./Data_final/data_ligue1.csv\",\"a\")    \n",
    "    merged_F.write('Season,Journee,HomeTeam,AwayTeam,FTHG,FTAG,URL\\n')  \n",
    "    for num in range(season1,season2+1):\n",
    "        ff = open(\"season_\"+str(season1)+\"_\"+str(season1+1)+\".csv\")\n",
    "        next(ff) # skip the header\n",
    "        for line in ff:\n",
    "            merged_F.write(f'{num}-{num+1},'+line)\n",
    "        ff.close() # not really needed\n",
    "    merged_F.close()\n",
    "\n",
    "def scrap_to_list(tree,xpth,scrape_type):\n",
    "    \"\"\"scrape type: 1 .text, 2 .attrib[\"alt\"]\"\"\"\n",
    "    scrape_list = tree.xpath(xpth)\n",
    "    list_out = []\n",
    "    if scrape_type == 1:\n",
    "        for each in scrape_list:\n",
    "            list_out.append(each.text.strip())\n",
    "    if scrape_type == 2:\n",
    "        for each in scrape_list:\n",
    "            list_out.append(each.attrib[\"alt\"])        \n",
    "    return list_out\n",
    "\n",
    "# Authentification pour utiliser API twitter\n",
    "def auth_cpt_twtr(consumer_key,consumer_secret,access_token,access_secret):\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "        print('Successful Authentication')\n",
    "        return api\n",
    "    except:\n",
    "        print('Failed authentication')\n",
    "        exit()\n",
    "\n",
    "# Recuperer les tweets par un requette (query)\n",
    "def get_tweets(query, api, bucket_url, count = 200):\n",
    "    q=str(query)\n",
    "    fetched_tweets = api.user_timeline(screen_name=q, count = count)\n",
    "    clmns = ['Tweet', 'Date de creation', 'hashtags']\n",
    "    data_tweets=[]\n",
    "    # analyser tweets une par une\n",
    "    for tweet in fetched_tweets:\n",
    "        # Ajouter les tweets à la dataframe\n",
    "        oneline_text = str.join(\" \", tweet.text.splitlines())\n",
    "        data_tweets.append([oneline_text, tweet.created_at, tweet.entities[\"hashtags\"]])\n",
    "    df = pd.DataFrame(data_tweets, columns=clmns)\n",
    "    df.to_csv(r'{}{}.csv'.format(bucket_url,query), index=False)\n",
    "    \n",
    "def images_downloader(media_files, directory):\n",
    "    path_downloaded= set()\n",
    "    for media_file in media_files:\n",
    "        path_downloaded.add(wget.download(media_file, directory))\n",
    "    return path_downloaded\n",
    "\n",
    "def get_photo_user(at_user,api):\n",
    "    # Definir un class à partir d'une function qui\n",
    "    @classmethod\n",
    "    def parse(cls, api, raw):\n",
    "        status = cls.first_parse(api, raw)\n",
    "        setattr(status, 'json', json.dumps(raw))\n",
    "        return status\n",
    "\n",
    "    # You need to do it for all the models you need\n",
    "    # Status() is the data model for a tweet\n",
    "    tweepy.models.Status.first_parse = tweepy.models.Status.parse\n",
    "    tweepy.models.Status.parse = parse\n",
    "    # User() is the data model for a user profil\n",
    "    tweepy.models.User.first_parse = tweepy.models.User.parse\n",
    "    tweepy.models.User.parse = parse\n",
    "    \n",
    "    # This code stores all the tweets by a specific user in the variable tweets. \n",
    "    # Now, we are ready to filter those with images. In our case, we want \n",
    "    # 200 tweets which are directly created by the user (i.e., No retweets nor replies)\n",
    "    tweets = api.user_timeline(screen_name=at_user,\n",
    "    count=1, include_rts=False,\n",
    "    exclude_replies=True)\n",
    "    last_id = tweets[-1].id\n",
    "    while (True):\n",
    "        more_tweets = api.user_timeline(screen_name=at_user,\n",
    "        since_id=\"2022-06-01\",\n",
    "        count=1, include_rts=False,\n",
    "        exclude_replies=True,\n",
    "        max_id=last_id-1)\n",
    "        # There are no more tweets\n",
    "        if (len(more_tweets) == 0):\n",
    "            break\n",
    "        else:\n",
    "            last_id = more_tweets[-1].id-1\n",
    "            tweets = tweets + more_tweets\n",
    "\n",
    "    # Trouver les liens pour toutes les images dans chaque tweet\n",
    "    media_files = set()\n",
    "    for status in tweets:\n",
    "        media = status.entities.get('media', [])\n",
    "        if(len(media) > 0):\n",
    "            media_files.add(media[0]['media_url'])\n",
    "    return media_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3691a-5a2c-4aaa-9603-4dada920dfaa",
   "metadata": {},
   "source": [
    "## parametres des années à scrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435e364-6248-4b2e-8339-6ff691d50f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_dbt = 16\n",
    "currentDateTime = datetime.datetime.now()\n",
    "date = currentDateTime.date()\n",
    "yr_fin = int(date.strftime(\"%y\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6265677-b9c6-49b0-93fa-3924a04fd7c1",
   "metadata": {},
   "source": [
    "## creation d'une destination local\n",
    "1. verifier si le dossier existe sinon le creer temporairement\n",
    "2. telecharger les fichiers data en question dans le dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a85ee-7d75-4536-847e-c3b1e8686b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dir=\"./data_F1/\"\n",
    "if not os.path.exists(s_dir):\n",
    "    os.mkdir(s_dir)\n",
    "for i in range(yr_dbt,yr_fin):\n",
    "    urllib.request.urlretrieve(f\"https://www.football-data.co.uk/mmz4281/{i}{i+1}/F1.csv\", f\"./{s_dir}F1_{i}_{i+1}.csv\")\n",
    "#Div\tDate\tTime\tHomeTeam\tAwayTeam\tFTHG\tFTAG\tFTR\tHTHG\tHTAG\tHTR\tHS\tAS\tHST\tAST\tHF\tAF\tHC\tAC\tHY\tAY\tHR\tAR\tB365H\tB365D\tB365A\tBWH\tBWD\tBWA\tIWH\tIWD\tIWA\tPSH\tPSD\tPSA\tWHH\tWHD\tWHA\tVCH\tVCD\tVCA\tMaxH\tMaxD\tMaxA\tAvgH\tAvgD\tAvgA\tB365>2.5\tB365<2.5\tP>2.5\tP<2.5\tMax>2.5\tMax<2.5\tAvg>2.5\tAvg<2.5\tAHh\tB365AHH\tB365AHA\tPAHH\tPAHA\tMaxAHH\tMaxAHA\tAvgAHH\tAvgAHA\tB365CH\tB365CD\tB365CA\tBWCH\tBWCD\tBWCA\tIWCH\tIWCD\tIWCA\tPSCH\tPSCD\tPSCA\tWHCH\tWHCD\tWHCA\tVCCH\tVCCD\tVCCA\tMaxCH\tMaxCD\tMaxCA\tAvgCH\tAvgCD\tAvgCA\tB365C>2.5\tB365C<2.5\tPC>2.5\tPC<2.5\tMaxC>2.5\tMaxC<2.5\tAvgC>2.5\tAvgC<2.5\tAHCh\tB365CAHH\tB365CAHA\tPCAHH\tPCAHA\tMaxCAHH\tMaxCAHA\tAvgCAHH\tAvgCAHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7da3d-b4be-4e49-9fb6-5d88685928a4",
   "metadata": {},
   "source": [
    "## creation un seul fichier data pour toutes les données scrappées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708664b5-3f77-4426-8149-2df891b8b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged =open(s_dir+\"F1.csv\",\"w\")    \n",
    "#merged.write('HomeTeam,AwayTeam,FTHG,FTAG\\n')  \n",
    "for yr in range(16,22):\n",
    "    if yr<19 :\n",
    "        f = open(f\"{s_dir}F1_{yr}_{yr+1}.csv\")\n",
    "        freading = csv.reader(f,delimiter=',')\n",
    "        next(freading) # skip the header\n",
    "        for ligne in freading:\n",
    "            merged.write(ligne[2]+','+ligne[3]+','+ligne[4]+','+ligne[5]+'\\n')\n",
    "        f.close() # not really needed\n",
    "    else:\n",
    "        f = open(f\"{s_dir}F1_{yr}_{yr+1}.csv\")\n",
    "        freading = csv.reader(f,delimiter=',')\n",
    "        next(freading) # skip the header\n",
    "        for ligne in freading:\n",
    "            merged.write(ligne[3]+','+ligne[4]+','+ligne[5]+','+ligne[6]+'\\n')\n",
    "        f.close() # not really needed\n",
    "\n",
    "merged.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b0cee-8e7a-47b5-902d-8e035fbe0cd4",
   "metadata": {},
   "source": [
    "## Deplacer le BDD final (le fichier F1 avec les fichiers telechargés) à notre bucket dans le chemin destiné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e580f9-6cc4-4b13-b024-3cec7cfa0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local = f\"{s_dir}F1.csv\"\n",
    "path_bucket = \"gs://bucket-git-m2i/DB/historique\"\n",
    "subprocess.call(f\"gsutil cp {path_local} {path_bucket}\", shell=True)\n",
    "for i in range(yr_dbt,yr_fin):\n",
    "    subprocess.call(f\"gsutil cp ./{s_dir}F1_{i}_{i+1}.csv {path_bucket}\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53eda5-5405-4035-8c42-576fe2ffc388",
   "metadata": {},
   "source": [
    "## Liberer l'espace local par supprimer le dossier creer temporairement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42bb91-e176-4a08-b879-89c5d0897483",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r $s_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a4509-65ea-4be1-8e0b-99286a9d696a",
   "metadata": {},
   "source": [
    "## Scrap du calendrier des matchs à venir -> construction d'un fichier CSV par journée qui liste les 10 matchs (home, away, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf9d9b01-eec9-4445-97c0-0a00ceedca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# journée à scrapper\n",
    "for journee in range(1,39): # journée de 1 à 23 (index final = 24-1)\n",
    "    # url de la site à scrapper\n",
    "    url_a_scrapper = f\"https://www.ligue1.fr/calendrier-resultats?seasonId=2022-2023&matchDay={journee}\"\n",
    "    # url principale de la site\n",
    "    url_base = \"https://www.ligue1.fr\"\n",
    "    # request pour avoir le corps de code de la page à scrapper\n",
    "    resp = requests.get(url_a_scrapper)\n",
    "    # trier la contnue du code de la page comme un arbre de html\n",
    "    tree = html.fromstring(resp.content)\n",
    "\n",
    "    # scrape toutes les id des matchs dans la page\n",
    "    match_id = []\n",
    "    match_id_code = tree.xpath('/html/body/main/div[3]/div[2]/div/div[2]/ul[*]/li/a/div[2]')\n",
    "    for each_id in match_id_code:\n",
    "        # if va chercher dans chaque balise a id son contenu \n",
    "        # et l'ajouter dans la liste de match_id\n",
    "        match_id.append(each_id.attrib[\"id\"])\n",
    "\n",
    "    # recuperer les elements à chercher par leurs xpath\n",
    "    equipe_home = []\n",
    "    equipe_away = []\n",
    "    match_href = []\n",
    "    match_url = []\n",
    "    i=0\n",
    "    for each_id in match_id:\n",
    "        # le resultat de tree.xpath sera en forme de liste \n",
    "        # il faut index[0] et .text pour recuperer le contenu\n",
    "        equipe_home.append(nom_normalizer(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a/div[1]/div/span[1]')[0].text))\n",
    "        equipe_away.append(nom_normalizer(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a/div[3]/div/span[1]')[0].text))\n",
    "        # Afin d'avoir acceder à la page de detailles de match\n",
    "        # on scrape le lien de chaque match et on l'ajout à la url_base\n",
    "        # afin de deviner un lien complet\n",
    "        # ici le resultat de tree.xpath a plusieurs attributes\n",
    "        # donc on choisis l'attribue concerné par [0].attrib[\"href\"]\n",
    "        # href c'est l'attribue nous interesse\n",
    "        match_href.append(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a'))\n",
    "        match_url.append(url_base + match_href[i][0].attrib[\"href\"])\n",
    "        i+=1\n",
    "#     for i in range(10):\n",
    "#         print(i+1,\" - \",equipe_home[i],\"\\t\", equipe_away[i],\"\\n url: \", match_url[i],\"\\n\")\n",
    "\n",
    "    # zipper les informations des listes dans un dataframe\n",
    "    # avec 3 columns correspondant à chaque liste\n",
    "    # et les sauvegarder dans un fichier CSV\n",
    "    df_journee_1 = pd.DataFrame(list(zip(equipe_home, equipe_away, match_url)), \n",
    "                        columns =['Equipe HOME', 'Equipe AWAY', 'URL']) \n",
    "    df_journee_1.to_csv(f'gs://bucket-git-m2i/DB/calendrier/journee_{journee}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cd5f6-22a3-488f-9481-e544838f8886",
   "metadata": {},
   "source": [
    "## Merging des csv issus du scrap calendrier -> construction d'un csv qui comprend la journée, home, away et l'url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e3ce7c-4891-4534-9871-3e7a2a29dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged =file_io.FileIO(\"gs://bucket-git-m2i/DB/calendrier/merging.csv\",mode=\"w\")    \n",
    "merged.write('Journée,Equipe_Home,Equipe_Away,URL\\n')  \n",
    "for num in range(1,39):\n",
    "    f = file_io.FileIO(\"gs://bucket-git-m2i/DB/calendrier/journee_\"+str(num)+\".csv\",mode=\"r\")\n",
    "    next(f) # skip the header\n",
    "    for line in f:\n",
    "        merged.write(f'{num},'+str(line))\n",
    "    f.close() # not really needed\n",
    "merged.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181a93f-9907-4833-9b0e-c9f9acfa7113",
   "metadata": {},
   "source": [
    "## Merging des csv contenant les résultats historiques des 5 dernières saisons -> csv comprenant home, away, score_home, score_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d27f49-ba1b-43e5-acf4-dbc6d169edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_F =file_io.FileIO(\"gs://bucket-git-m2i/DB/historique/merging_F.csv\",mode=\"w\")    \n",
    "merged_F.write('HomeTeam;AwayTeam;FTHG;FTAG\\n')  \n",
    "for num in range(1,3):\n",
    "    ff = file_io.FileIO(\"gs://bucket-git-m2i/DB/historique/F\"+str(num)+\".csv\",mode=\"r\")\n",
    "    next(ff) # skip the header\n",
    "    for line in ff:\n",
    "        merged_F.write(line)\n",
    "    ff.close() # not really needed\n",
    "merged_F.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6305d6-7cdb-418b-bdff-7ac0b9f854c9",
   "metadata": {},
   "source": [
    "## Se basant sur merging historique -> calcul (dans variables) des statistiques de la ligue (toutes équipes confondues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14054aa8-3b0f-4330-a7a1-1fe1f48007f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://bucket-git-m2i/DB/historique/merging_F.csv', sep=';')\n",
    "### normalizer tous les noms des equipes dans le dataframe\n",
    "# Home \n",
    "df['HomeTeam'] = df['HomeTeam'].str.strip().str.upper()\n",
    "df['HomeTeam'] = df['HomeTeam'].apply(nom_normalizer)\n",
    "# Away\n",
    "df['AwayTeam'] = df['AwayTeam'].str.strip().str.upper()\n",
    "df['AwayTeam'] = df['AwayTeam'].apply(nom_normalizer)\n",
    "\n",
    "# print(df)\n",
    "\n",
    "home_total = df[['FTHG']].sum()[0]\n",
    "home_average = df[['FTHG']].mean(numeric_only=True)[0]\n",
    "nb_of_matches = df[['HomeTeam']].count()[0]\n",
    "# print(home_total)\n",
    "# print(home_average)\n",
    "# print(nb_of_matches)\n",
    "\n",
    "away_total = df[['FTAG']].sum()[0]\n",
    "away_average = df[['FTAG']].mean(numeric_only=True)[0]\n",
    "# print(away_total)\n",
    "# print(away_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901398ad-df79-4e66-bc5c-90f5daa2e9fb",
   "metadata": {},
   "source": [
    "## Calcul d'un scoring de chaque équipe (indépendemment de son adversaire) selon away/home relatif aux buts encaissés et marqués -> création csv reprenant ces résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6209100-8535-4be4-b9cf-bce71bea6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_io.FileIO('gs://bucket-git-m2i/DB/calendrier/merging.csv',mode=\"r\") as calendrier:\n",
    "    csv_reader = csv.reader(calendrier)\n",
    "    with file_io.FileIO('gs://bucket-git-m2i/DB/golden/final-results.csv', mode='w') as new_file:\n",
    "        # on ouvre un csv pour sauvegarder les resultats\n",
    "        csv_writer = csv.writer(new_file)                \n",
    "\n",
    "        for line in calendrier:# line est un chaine des caracs\n",
    "            data_line = line.split(sep=',') # separer les donnée\n",
    "            domicile = nom_normalizer(data_line[1].strip().upper()) # recuperer l'equipe domicile\n",
    "            visiteur = nom_normalizer(data_line[2].strip().upper()) # recuperer l'equipe visiteur\n",
    "            \n",
    "            \n",
    "            team_home_df = df[df['HomeTeam']==domicile]\n",
    "            team_home_total = team_home_df[['FTHG']].sum()[0]\n",
    "            team_home_average = team_home_df[['FTHG']].mean()[0]\n",
    "            home_conceed_total = team_home_df[['FTAG']].sum()[0]\n",
    "            home_conceed_average = team_home_df[['FTAG']].mean()[0]\n",
    "            nb_matches_home = team_home_df[['FTAG']].count()[0]\n",
    "            home_attack_str = (team_home_average / home_average)\n",
    "            home_defence_str = (home_conceed_average / away_average)\n",
    "\n",
    "            team_away_df = df[df['AwayTeam']==visiteur]\n",
    "            away_score_total = team_away_df[['FTAG']].sum()[0]\n",
    "            away_score_average = team_away_df[['FTAG']].mean()[0]\n",
    "            away_conceed_total = team_away_df[['FTHG']].sum()[0]\n",
    "            away_conceed_average = team_away_df[['FTHG']].mean()[0]\n",
    "            nb_matches_away = team_away_df[['FTHG']].count()[0]\n",
    "            away_attack_str = (away_score_average / away_average)\n",
    "            away_defence_str = (away_conceed_average / home_average)\n",
    "\n",
    "            #csv_writer.writerow(str(away_attack_str)+','+str(away_defence_str)+'\\n')\n",
    "            new_file.write(str(domicile)+','+str(visiteur)+','+str(home_attack_str).strip()+','+str(home_defence_str).strip()+','+str(away_attack_str).strip()+','+str(away_defence_str).strip()+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c5c53-bba3-4476-a70a-1f42a8331223",
   "metadata": {},
   "source": [
    "## Calcul des probabilités du résultat (draw / home_win / away_win) -> création csv reprenant ces résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029c457c-ae3f-4853-8a65-2ba2e350dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_io.FileIO('gs://bucket-git-m2i/DB/golden/final-results.csv', mode='r') as matchs:\n",
    "    csv_reader = csv.reader(matchs)\n",
    "    with file_io.FileIO('gs://bucket-git-m2i/DB/golden/match-prob.csv', mode='w') as probs:\n",
    "        # on ouvre un csv pour sauvegarder les resultats\n",
    "        csv_writer = csv.writer(probs) \n",
    "        for line in matchs: # line est un chaine des caracs\n",
    "            data_line = line.split(sep=',') # separer les donnée\n",
    "            domicile = nom_normalizer(data_line[0].strip().upper()) # recuperer l'equipe domicile\n",
    "            visiteur = nom_normalizer(data_line[1].strip().upper()) # recuperer l'equipe visiteur\n",
    "            away_attack_str = data_line[2].strip()\n",
    "            away_defence_str = data_line[3].strip()\n",
    "            home_defence_str = data_line[4].strip()\n",
    "            home_attack_str = data_line[5].strip()\n",
    "\n",
    "            home_expect = float(home_attack_str) * float(away_defence_str) * float(home_average)\n",
    "            away_expect = float(away_attack_str) * float(home_defence_str) * float(away_average)\n",
    "            def poisson_probability(l, x):\n",
    "                probability = ((l**x) * math.exp(-l)) / math.factorial(x)\n",
    "                return probability*100\n",
    "            home_goals_prob = []\n",
    "            for i in range(8):\n",
    "                expect = poisson_probability(home_expect, i)\n",
    "                home_goals_prob.append(expect)\n",
    "            home_goals_probs = np.round(home_goals_prob,2)\n",
    "            away_goals_prob = []\n",
    "            for i in range(8):\n",
    "                expect = poisson_probability(away_expect, i)\n",
    "                away_goals_prob.append(expect)  \n",
    "            away_goals_probs = np.round(away_goals_prob,2) \n",
    "            p = {'Home0':[((home_goals_probs[0]*away_goals_probs[0])/100), ((home_goals_probs[0]*away_goals_probs[1])/100), ((home_goals_probs[0]*away_goals_probs[2])/100), ((home_goals_probs[0]*away_goals_probs[3])/100), ((home_goals_probs[0]*away_goals_probs[4])/100), ((home_goals_probs[0]*away_goals_probs[5])/100),((home_goals_probs[0]*away_goals_probs[6])/100),((home_goals_probs[0]*away_goals_probs[7])/100)], \n",
    "                'Home1':[((home_goals_probs[1]*away_goals_probs[0])/100), ((home_goals_probs[1]*away_goals_probs[1])/100), ((home_goals_probs[1]*away_goals_probs[2])/100), ((home_goals_probs[1]*away_goals_probs[3])/100), ((home_goals_probs[1]*away_goals_probs[4])/100), ((home_goals_probs[1]*away_goals_probs[5])/100),((home_goals_probs[1]*away_goals_probs[6])/100),((home_goals_probs[1]*away_goals_probs[7])/100)], \n",
    "                'Home2':[((home_goals_probs[2]*away_goals_probs[0])/100), ((home_goals_probs[2]*away_goals_probs[1])/100), ((home_goals_probs[2]*away_goals_probs[2])/100), ((home_goals_probs[2]*away_goals_probs[3])/100), ((home_goals_probs[2]*away_goals_probs[4])/100), ((home_goals_probs[2]*away_goals_probs[5])/100),((home_goals_probs[2]*away_goals_probs[6])/100),((home_goals_probs[2]*away_goals_probs[7])/100)], \n",
    "                'Home3':[((home_goals_probs[3]*away_goals_probs[0])/100), ((home_goals_probs[3]*away_goals_probs[1])/100), ((home_goals_probs[3]*away_goals_probs[2])/100), ((home_goals_probs[3]*away_goals_probs[3])/100), ((home_goals_probs[3]*away_goals_probs[4])/100), ((home_goals_probs[3]*away_goals_probs[5])/100),((home_goals_probs[3]*away_goals_probs[6])/100),((home_goals_probs[3]*away_goals_probs[7])/100)], \n",
    "                'Home4':[((home_goals_probs[4]*away_goals_probs[0])/100), ((home_goals_probs[4]*away_goals_probs[1])/100), ((home_goals_probs[4]*away_goals_probs[2])/100), ((home_goals_probs[4]*away_goals_probs[3])/100), ((home_goals_probs[4]*away_goals_probs[4])/100), ((home_goals_probs[4]*away_goals_probs[5])/100),((home_goals_probs[4]*away_goals_probs[6])/100),((home_goals_probs[4]*away_goals_probs[7])/100)], \n",
    "                'Home5':[((home_goals_probs[5]*away_goals_probs[0])/100), ((home_goals_probs[5]*away_goals_probs[1])/100), ((home_goals_probs[5]*away_goals_probs[2])/100), ((home_goals_probs[5]*away_goals_probs[3])/100), ((home_goals_probs[5]*away_goals_probs[4])/100), ((home_goals_probs[5]*away_goals_probs[5])/100),((home_goals_probs[5]*away_goals_probs[6])/100),((home_goals_probs[5]*away_goals_probs[7])/100)],\n",
    "                'Home6':[((home_goals_probs[6]*away_goals_probs[0])/100), ((home_goals_probs[6]*away_goals_probs[1])/100), ((home_goals_probs[6]*away_goals_probs[2])/100), ((home_goals_probs[6]*away_goals_probs[3])/100), ((home_goals_probs[6]*away_goals_probs[4])/100), ((home_goals_probs[6]*away_goals_probs[5])/100),((home_goals_probs[6]*away_goals_probs[6])/100),((home_goals_probs[6]*away_goals_probs[7])/100)],\n",
    "                'Home7':[((home_goals_probs[7]*away_goals_probs[0])/100), ((home_goals_probs[7]*away_goals_probs[1])/100), ((home_goals_probs[7]*away_goals_probs[2])/100), ((home_goals_probs[7]*away_goals_probs[3])/100), ((home_goals_probs[7]*away_goals_probs[4])/100), ((home_goals_probs[7]*away_goals_probs[5])/100),((home_goals_probs[7]*away_goals_probs[6])/100),((home_goals_probs[7]*away_goals_probs[7])/100)]}\n",
    "            probability = pd.DataFrame(p, index=['away0','away1','away2', 'away3', 'away4', 'away5', 'away6', 'away7'])\n",
    "            nump = probability.to_numpy()\n",
    "            draw_prob = np.trace(nump)\n",
    "            Home_win_prob = (np.trace(nump, offset = 1))+(np.trace(nump, offset = 2))+(np.trace(nump, offset = 3))+(np.trace(nump, offset = 4))+(np.trace(nump, offset = 5))+(np.trace(nump, offset = 6))\n",
    "            away_win_prob = (np.trace(nump, offset = -1))+(np.trace(nump, offset = -2))+(np.trace(nump, offset = -3))+(np.trace(nump, offset = -4))+(np.trace(nump, offset = -5))+(np.trace(nump, offset = -6))\n",
    "            probs.write(domicile+','+visiteur+','+str(draw_prob).strip()+','+str(Home_win_prob)+','+str(away_win_prob)+','+'\\n')\n",
    "matchs.close()\n",
    "probs.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
