{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e92976-2559-4e72-8d14-1c2dd4477ef1",
   "metadata": {},
   "source": [
    "## ==> PIPELINE QUI CONSTRUIT F1 et F2, les deux bases de données des stats historiques pour la ligue1 et la ligue2, mise à jour jusqu'aux matches recement joués pendant la season en cours\n",
    "### ==> à lancer à la date d'après de chaque journée <=="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51971457-c123-40ee-8be7-df53ce3a67e1",
   "metadata": {},
   "source": [
    "### IMPORTER les Libraries nécessaires au fonctionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6387a8d-6ae8-4bb8-a5e2-1b594535aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import urllib.request\n",
    "import subprocess\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6988a-8a47-49c1-9774-7a281b556340",
   "metadata": {},
   "source": [
    "## Panneau des parametres à regler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1fe6d-1d29-4439-96e7-a34f394cb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'année où le scrapping à va commencer\n",
    "yr_dbt = 19\n",
    "\n",
    "# l'année où le scrpping va se terminer (année en cours) le +1 est pour prendre en compte le fin de range()\n",
    "currentDateTime = datetime.datetime.now()\n",
    "date = currentDateTime.date()\n",
    "yr_fin = int(date.strftime(\"%y\"))+1 \n",
    "\n",
    "# les chemins des dossiers dans le bucket\n",
    "path_bucket_lake = \"gs://bucket-git-m2i/DB/historique/\"\n",
    "path_bucket_gold = \"gs://bucket-git-m2i/DB/golden/\" \n",
    "\n",
    "# table_id to the ID of the table to create.\n",
    "table_id = \"micro-atrium-360309.m2i_pronostics.merging-F\"\n",
    "\n",
    "# pour execution schedulée, extraction de l'ID du projet temporaire créé\n",
    "project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"] #si execution schedulée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2c9f6-fee2-4b1d-b0f8-30524d437d62",
   "metadata": {},
   "source": [
    "## Scrapper les données des stats historiques et les stocker dans une destination locale\n",
    "1. verifier si le dossier existe sinon le creer temporairement\n",
    "2. telecharger les fichiers data en question dans le dossier / pour chaque ligue\n",
    "3. creation un seul fichier data pour toutes les données scrappées / pour chaque ligue\n",
    "4. Deplacer le BDD final (le fichier F1 avec les fichiers telechargés) à notre bucket dans le chemin destiné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824aa6df-aa6a-48af-aa0a-6bcbe7c1b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ligue in range(1,3):\n",
    "    # 1.\n",
    "    s_dir=f\"./data_ligue_{ligue}/\"\n",
    "    if not os.path.exists(s_dir):\n",
    "        os.mkdir(s_dir)\n",
    "    # 2.\n",
    "    for i in range(yr_dbt,yr_fin):\n",
    "        urllib.request.urlretrieve(f\"https://www.football-data.co.uk/mmz4281/{i}{i+1}/F{ligue}.csv\", f\"{s_dir}F{ligue}_{i}_{i+1}.csv\")\n",
    "    # 3.\n",
    "    merged =open(f\"{s_dir}F{ligue}.csv\",\"w\")    \n",
    "    merged.write('Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\\n')  \n",
    "    for yr in range(yr_dbt,yr_fin):\n",
    "        f = open(f\"{s_dir}F{ligue}_{yr}_{yr+1}.csv\")\n",
    "        next(f)\n",
    "        for ligne in f.readlines():\n",
    "            merged.write(ligne)\n",
    "        f.close() # not really needed\n",
    "    merged.close()\n",
    "    \n",
    "    # 4.\n",
    "    path_local = f\"{s_dir}F{ligue}.csv\"\n",
    "    subprocess.call(f\"gsutil cp {path_local} {path_bucket_lake}F{ligue}.csv\", shell=True)\n",
    "    for i in range(yr_dbt,yr_fin):\n",
    "        subprocess.call(f\"gsutil cp ./{s_dir}F{ligue}_{i}_{i+1}.csv {path_bucket_lake}\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacee11f-4756-441e-89a4-9e38e66a9fb4",
   "metadata": {},
   "source": [
    "## Merging des csv contenant les résultats historiques des 5 dernières saisons -> csv comprenant home, away, score_home, score_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer un path local pour le fichier où on va merger les F1 et F2\n",
    "path_local = f\"./data_ligue_1/merging_F.csv\"\n",
    "\n",
    "# on ouvre le fichier\n",
    "merged_F =open(path_local,\"a\")\n",
    "\n",
    "# on commence la construction par la tête des titres de columns\n",
    "merged_F.write('Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\\n')  \n",
    "\n",
    "# en boucle for on ouvre F1 et F2 et on copie chaque ligne sauf la tête\n",
    "for ligue in range(1,3):\n",
    "    # ouvrir le fichier F{ligue}=F1 et F2\n",
    "    f = open(f\"./data_ligue_{ligue}/F{ligue}.csv\")\n",
    "    # on evite premier ligne\n",
    "    next(f)\n",
    "    # dans le fichier F ouvert on va copier (write) chaque ligne el l'envoyer vers merged_F\n",
    "    for ligne in f.readlines():\n",
    "        merged_F.write(ligne)\n",
    "    f.close() # not really needed\n",
    "merged_F.close()\n",
    "\n",
    "# copier le fichier final merging_F vers le bucket dans le dossier golden\n",
    "subprocess.call(f\"gsutil cp {path_local} {path_bucket_gold}merging_F.csv\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fcaa7-47b1-47f8-ad25-a76cea06d925",
   "metadata": {},
   "source": [
    "## Liberer l'espace local par supprimer le dossier creer temporairement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e7837-b9e8-4047-ad67-6a29b46327fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ligue in range (1,3):\n",
    "    s_dir = f\"./data_ligue_{ligue}/\"\n",
    "    !rm -r $s_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc5cf5-249b-481d-a3d4-e085d076c2d1",
   "metadata": {},
   "source": [
    "### Le csv créé (merging_F) est importé dans un dataset bigQuery (en écrasant les donnnées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74f9c4-8937-45a4-83d5-7c0a088ccf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a BigQuery client object.\n",
    "#client = bigquery.Client() #si execution manuelle\n",
    "client = bigquery.Client(project_number) #si execution schedulée\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    autodetect=True,\n",
    "    # The source format defaults to CSV, so the line below is optional.\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    ")\n",
    "uri = f\"{path_bucket_gold}merging_F.csv\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)  # Make an API request.\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
