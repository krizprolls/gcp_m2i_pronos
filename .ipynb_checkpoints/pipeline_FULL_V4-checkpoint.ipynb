{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e92976-2559-4e72-8d14-1c2dd4477ef1",
   "metadata": {},
   "source": [
    "### --> PIPELINE QUI CONSTRUIT F1 et F2, ajoute à l'historique les matchs joués de la saison en cours et copie les golden csv dans BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51971457-c123-40ee-8be7-df53ce3a67e1",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6387a8d-6ae8-4bb8-a5e2-1b594535aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d8e64-c2ba-43be-9a49-24e5f3b96479",
   "metadata": {},
   "source": [
    "### DEFINITION FONCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11078ba-564f-42af-810e-8446ce261967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nom_normalizer(equipe_a_normaliser):\n",
    "    teams = {\n",
    "        'OLYMPIQUE LYONNAIS': ['OLYMPIQUE LYONNAIS', 'LYON', 'OL', 'O.L.'], \n",
    "        'AC AJACCIO': ['AC AJACCIO', 'AJACCIO', 'ACA', 'A.C.A', 'ATHLETIC CLUB AJACCIEN','A-C AJACCIO','A.C AJACCIO'],\n",
    "        'RC STRASBOURG ALSACE' : ['STRASBOURG','RACING CLUB DE STRASBOURG ALSACE', 'RC STRASBOURG','R.C. STRASBOURG','R.C.S.','RCS', 'R.C.S.A.', 'RC STRASBOURG ALSACE', 'R.C. STRASBOURG ALSACE'], \n",
    "        'AS MONACO': ['AS MONACO', 'MONACO', 'ASSOCIATION SPORTIVE DE MONACO FOOTBALL CLUB', 'AS MONACO FC', 'A.S MONACO F.C', 'ASM'], \n",
    "        'CLERMONT FOOT 63': ['CLERMONT FOOT 63', 'CLERMONT-FERRAND', 'CF63', 'C.F.63', 'C-F 63', 'CLERMONT', 'CLERMONT FOOT'], \n",
    "        'PARIS SAINT GERMAIN': ['PARIS SAINT GERMAIN', 'PARIS-SAINT-GERMAIN FOOTBALL CLUB', 'PSG', 'P.S.G.', 'PARIS SG', 'PARIS-SG', 'PARIS SAINT-GERMAIN', 'PARIS SAINT-GERMAIN FOOTBALL CLUB', 'PARIS SAINT-GERMAIN FC'], \n",
    "        'TOULOUSE FC' : ['TOULOUSE', 'TOULOUSE FOOTBALL CLUB', 'TFC', 'T.F.C.', 'TOULOUSE FC', 'TOULOUSE F.C.'], \n",
    "        'OGC NICE': ['NICE', \"OLYMPIQUE GYMNASTE CLUB NICE COTE D'AZUR\", 'OGC NICE', 'O.G.C. NICE', \"OGC NICE COTE D'AZUR\", \"O.G.C. NICE COTE D'AZUR\", 'OLYMPIQUE GYMNASTE CLUB NICE'], \n",
    "        'ANGERS SCO': ['ANGERS SCO', 'ANGERS', \"SCO D'ANGERS\", 'SCOA', \"ANGERS SPORTING CLUB DE L'OUEST\", 'SCO ANGERS', 'ANGERS-SCO', 'ANGERS S.C.O.', 'S.C.O. ANGERS'], \n",
    "        'FC NANTES' : ['FC NANTES', 'NANTES', 'FOOTBALL CLUB DE NANTES', 'F.C. NANTES', 'FCN', 'F.C.N.'], \n",
    "        'RC LENS': ['RC LENS', 'LENS', 'RACING CLUB DE LENS', 'RCL', 'R.C.L.', 'R.C. LENS'], \n",
    "        'STADE BRESTOIS': ['STADE BRESTOIS', 'BREST', 'STADE BRESTOIS 29', 'SB29', 'S.B.29', 'S.B. 29'], \n",
    "        'LOSC LILLE': ['LOSC LILLE', 'LILLE', 'LOSC', 'LILLE OLYMPIQUE SPORTING CLUB', 'L.O.S.C.', 'L.O.S.C. LILLE'], \n",
    "        'AJ AUXERRE': ['AJ AUXERRE', 'AUXERRE', 'AJA', 'ASSOCIATION DE LA JEUNESSE AUXERROISE', 'A.J.AUXERRE', 'A.J AUXERRE'], \n",
    "        'MONTPELLIER HERAULT SC': ['MONTPELLIER HÃ‰RAULT SC', 'MONTPELLIER HÉRAULT SC','MONTPELLIER HÉRAULT', 'MONTPELLIER', 'MONTPELLIER-HERAULT SPORT CLUB', 'MONTPELLIER HERAULT SPORT CLUB', 'MONTPELLIER HSC', 'MHSC', 'MONTPELLIER-HERAULT SC', 'MONTPELLIER-HÉRAULT SC', 'MONTPELLIER-HERAULT S.C.', 'M.H.S.C.', 'MONTPELLIER HERAULT SC', 'MONTPELLIER HÉRAULT SC', 'MONTPELLIER HERAULT S.C.'], \n",
    "        'ESTAC TROYES': ['TROYES', 'ESPERANCE SPORTIVE TROYES AUBE CHAMPAGNE', 'ESTAC', 'E.S.T.A.C.', 'ESTAC TROYES', 'E.S.T.A.C. TROYES'], \n",
    "        'STADE RENNAIS FC': ['RENNES', 'STADE RENNAIS FOOTBALL CLUB', 'STADE RENNAIS', 'STADE RENNAIS FC', 'S.R.F.C.', 'STADE RENNAIS F.C.'], \n",
    "        'FC LORIENT': ['FC LORIENT', 'LORIENT', 'FOOTBALL CLUB LORIENT BRETAGNE SUD', 'FCL', 'F.C.L.', 'F.C. LORIENT', 'FC LORIENT BRETAGNE SUD', 'FOOTBALL CLUB LORIENTAIS'], \n",
    "        'OLYMPIQUE DE MARSEILLE': ['OLYMPIQUE DE MARSEILLE', 'MARSEILLE', 'OM', 'O.M.'],\n",
    "        'STADE DE REIMS': ['STADE DE REIMS', 'REIMS', 'STADE REIMS'],\n",
    "        'DIJON': ['DIJON', 'DIJON FCO', 'DFCO', \"DIJON FOOTBALL COTE-D'OR\", \"DIJON FOOTBALL CÔTE-D'OR\"],\n",
    "        'SAINT ETIENNE': ['ST ETIENNE', 'ST-ETIENNE', 'ST ÉTIENNE', 'ST-ÉTIENNE', 'SAINT ETIENNE', 'SAINT-ETIENNE', 'SAINT ÉTIENNE', 'SAINT-ÉTIENNE', 'ASSE','AS SAINT ETIENNE', 'AS SAINT-ETIENNE', 'AS SAINT-ÉTIENNE', 'AS SAINT-ÉTIENNE'],\n",
    "        'BORDEAUX': ['BORDEAUX', 'FOOTBALL CLUB DES GIRONDINS DE BORDEAUX', 'FC GIRONDINS DE BORDEAUX', 'GIRONDINS DE BORDEAUX', 'FCG BORDEAUX', 'FOOTBALL CLUB GIRONDINS DE BORDEAUX'],\n",
    "        'METZ': ['METZ', 'FC METZ', 'FOOTBALL CLUB DE METZ', 'FOOTBALL CLUB METZ']}\n",
    "    resultat = equipe_a_normaliser\n",
    "    for equipe_name in teams:\n",
    "        if equipe_a_normaliser in teams[equipe_name]:\n",
    "            return equipe_name\n",
    "    return resultat\n",
    "\n",
    "def poisson_probability(l, x):\n",
    "                probability = ((l**x) * math.exp(-l)) / math.factorial(x)\n",
    "                return probability*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6988a-8a47-49c1-9774-7a281b556340",
   "metadata": {},
   "source": [
    "## parametres des années à scrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22c1fe6d-1d29-4439-96e7-a34f394cb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligue = 1\n",
    "yr_dbt = 19\n",
    "currentDateTime = datetime.datetime.now()\n",
    "date = currentDateTime.date()\n",
    "yr_fin = int(date.strftime(\"%y\"))+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2c9f6-fee2-4b1d-b0f8-30524d437d62",
   "metadata": {},
   "source": [
    "## creation d'une destination local\n",
    "1. verifier si le dossier existe sinon le creer temporairement\n",
    "2. telecharger les fichiers data en question dans le dossier / pour chaque ligue\n",
    "3. creation un seul fichier data pour toutes les données scrappées / pour chaque ligue\n",
    "4. Deplacer le BDD final (le fichier F1 avec les fichiers telechargés) à notre bucket dans le chemin destiné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "824aa6df-aa6a-48af-aa0a-6bcbe7c1b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./data_ligue_1/F1.csv [Content-Type=text/csv]...\n",
      "/ [1 files][480.0 KiB/480.0 KiB]                                                \n",
      "Operation completed over 1 objects/480.0 KiB.                                    \n",
      "Copying file://././data_ligue_1/F1_19_20.csv [Content-Type=text/csv]...\n",
      "/ [1 files][122.1 KiB/122.1 KiB]                                                \n",
      "Operation completed over 1 objects/122.1 KiB.                                    \n",
      "Copying file://././data_ligue_1/F1_20_21.csv [Content-Type=text/csv]...\n",
      "/ [1 files][166.5 KiB/166.5 KiB]                                                \n",
      "Operation completed over 1 objects/166.5 KiB.                                    \n",
      "Copying file://././data_ligue_1/F1_21_22.csv [Content-Type=text/csv]...\n",
      "/ [1 files][167.3 KiB/167.3 KiB]                                                \n",
      "Operation completed over 1 objects/167.3 KiB.                                    \n",
      "Copying file://././data_ligue_1/F1_22_23.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 26.9 KiB/ 26.9 KiB]                                                \n",
      "Operation completed over 1 objects/26.9 KiB.                                     \n",
      "Copying file://./data_ligue_2/F2.csv [Content-Type=text/csv]...\n",
      "/ [1 files][483.7 KiB/483.7 KiB]                                                \n",
      "Operation completed over 1 objects/483.7 KiB.                                    \n",
      "Copying file://././data_ligue_2/F2_19_20.csv [Content-Type=text/csv]...\n",
      "/ [1 files][122.8 KiB/122.8 KiB]                                                \n",
      "Operation completed over 1 objects/122.8 KiB.                                    \n",
      "Copying file://././data_ligue_2/F2_20_21.csv [Content-Type=text/csv]...\n",
      "/ [1 files][166.4 KiB/166.4 KiB]                                                \n",
      "Operation completed over 1 objects/166.4 KiB.                                    \n",
      "Copying file://././data_ligue_2/F2_21_22.csv [Content-Type=text/csv]...\n",
      "/ [1 files][166.6 KiB/166.6 KiB]                                                \n",
      "Operation completed over 1 objects/166.6 KiB.                                    \n",
      "Copying file://././data_ligue_2/F2_22_23.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 30.7 KiB/ 30.7 KiB]                                                \n",
      "Operation completed over 1 objects/30.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "for ligue in range(1,3):\n",
    "    s_dir=f\"./data_ligue_{ligue}/\"\n",
    "    if not os.path.exists(s_dir):\n",
    "        os.mkdir(s_dir)\n",
    "    for i in range(yr_dbt,yr_fin):\n",
    "        urllib.request.urlretrieve(f\"https://www.football-data.co.uk/mmz4281/{i}{i+1}/F{ligue}.csv\", f\"{s_dir}F{ligue}_{i}_{i+1}.csv\")\n",
    "    #Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\n",
    "    merged =open(f\"{s_dir}F{ligue}.csv\",\"w\")    \n",
    "    merged.write('Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\\n')  \n",
    "    \n",
    "    for yr in range(yr_dbt,yr_fin):\n",
    "        f = open(f\"{s_dir}F{ligue}_{yr}_{yr+1}.csv\")\n",
    "        next(f)\n",
    "        for ligne in f.readlines():\n",
    "            merged.write(ligne)\n",
    "        f.close() # not really needed\n",
    "        \n",
    "    merged.close()\n",
    "    \n",
    "    path_local = f\"{s_dir}F{ligue}.csv\"\n",
    "    path_bucket = \"gs://bucket-git-m2i/DB/historique\"\n",
    "    subprocess.call(f\"gsutil cp {path_local} {path_bucket}\", shell=True)\n",
    "    for i in range(yr_dbt,yr_fin):\n",
    "        subprocess.call(f\"gsutil cp ./{s_dir}F{ligue}_{i}_{i+1}.csv {path_bucket}\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fcaa7-47b1-47f8-ad25-a76cea06d925",
   "metadata": {},
   "source": [
    "## Liberer l'espace local par supprimer le dossier creer temporairement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d0e7837-b9e8-4047-ad67-6a29b46327fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ligue in range (1,3):\n",
    "    s_dir = f\"./data_ligue_{ligue}/\"\n",
    "    !rm -r $s_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c7d42-6fe3-461a-9087-50d617ef2c7b",
   "metadata": {},
   "source": [
    "## Scrap du calendrier des matchs à venir -> construction d'un fichier CSV par journée qui liste les 10 matchs (home, away, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca66b4a-ddee-4ad1-a405-f3240cf627e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# journée à scrapper\n",
    "for journee in range(1,39): \n",
    "    # url de la site à scrapper\n",
    "    url_a_scrapper = f\"https://www.ligue1.fr/calendrier-resultats?seasonId=2022-2023&matchDay={journee}\"\n",
    "    # url principale de la site\n",
    "    url_base = \"https://www.ligue1.fr\"\n",
    "    # request pour avoir le corps de code de la page à scrapper\n",
    "    resp = requests.get(url_a_scrapper)\n",
    "    # trier la contnue du code de la page comme un arbre de html\n",
    "    tree = html.fromstring(resp.content)\n",
    "\n",
    "    # scrape toutes les id des matchs dans la page\n",
    "    match_id = []\n",
    "    match_id_code = tree.xpath('/html/body/main/div[3]/div[2]/div/div[2]/ul[*]/li/a/div[2]')\n",
    "    for each_id in match_id_code:\n",
    "        # if va chercher dans chaque balise a id son contenu \n",
    "        # et l'ajouter dans la liste de match_id\n",
    "        match_id.append(each_id.attrib[\"id\"])\n",
    "\n",
    "    # recuperer les elements à chercher par leurs xpath\n",
    "    equipe_home = []\n",
    "    equipe_away = []\n",
    "    match_href = []\n",
    "    match_url = []\n",
    "    i=0\n",
    "    for each_id in match_id:\n",
    "        # le resultat de tree.xpath sera en forme de liste \n",
    "        # il faut index[0] et .text pour recuperer le contenu\n",
    "        equipe_home.append(nom_normalizer(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a/div[1]/div/span[1]')[0].text))\n",
    "        equipe_away.append(nom_normalizer(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a/div[3]/div/span[1]')[0].text))\n",
    "        # Afin d'avoir acceder à la page de detailles de match\n",
    "        # on scrape le lien de chaque match et on l'ajout à la url_base\n",
    "        # afin de deviner un lien complet\n",
    "        # ici le resultat de tree.xpath a plusieurs attributes\n",
    "        # donc on choisis l'attribue concerné par [0].attrib[\"href\"]\n",
    "        # href c'est l'attribue nous interesse\n",
    "        match_href.append(tree.xpath(f'//*[@id=\"{each_id}_match-result\"]/a'))\n",
    "        match_url.append(url_base + match_href[i][0].attrib[\"href\"])\n",
    "        i+=1\n",
    "#     for i in range(10):\n",
    "#         print(i+1,\" - \",equipe_home[i],\"\\t\", equipe_away[i],\"\\n url: \", match_url[i],\"\\n\")\n",
    "\n",
    "    # zipper les informations des listes dans un dataframe\n",
    "    # avec 3 columns correspondant à chaque liste\n",
    "    # et les sauvegarder dans un fichier CSV\n",
    "    df_journee_1 = pd.DataFrame(list(zip(equipe_home, equipe_away, match_url)), \n",
    "                        columns =['Equipe HOME', 'Equipe AWAY', 'URL']) \n",
    "    df_journee_1.to_csv(f'gs://bucket-git-m2i/DB/calendrier/journee_{journee}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1d26d-f6a8-4efe-8c05-6c13fb37d5fa",
   "metadata": {},
   "source": [
    "## Merging des csv issus du scrap calendrier -> construction d'un csv qui comprend la journée, home, away et l'url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62f2c8c2-ca80-4ef6-8272-c4c7fab28e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged =file_io.FileIO(\"gs://bucket-git-m2i/DB/golden/merging_calendrier.csv\",mode=\"w\")    \n",
    "merged.write('Journée,Equipe_Home,Equipe_Away,URL\\n')  \n",
    "for num in range(1,39):\n",
    "    f = file_io.FileIO(\"gs://bucket-git-m2i/DB/calendrier/journee_\"+str(num)+\".csv\",mode=\"r\")\n",
    "    next(f) # skip the header\n",
    "    for line in f:\n",
    "        merged.write(f'{num},'+str(line))\n",
    "    f.close() # not really needed\n",
    "merged.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f70bd-ffd6-4a2c-90c0-1926d5270257",
   "metadata": {},
   "source": [
    "### Le csv créé (merging_calendrier) est importé dans un dataset bigQuery (en écrasant les donnnées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb936434-c307-4208-8503-6216447936a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 370 rows.\n"
     ]
    }
   ],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"micro-atrium-360309.m2i_pronostics.merging-calendrier\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    # The source format defaults to CSV, so the line below is optional.\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    ")\n",
    "uri = \"gs://bucket-git-m2i/DB/golden/merging_calendrier.csv\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)  # Make an API request.\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacee11f-4756-441e-89a4-9e38e66a9fb4",
   "metadata": {},
   "source": [
    "## Merging des csv contenant les résultats historiques des 5 dernières saisons -> csv comprenant home, away, score_home, score_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc6d854a-aa59-4cf3-9e0a-3998fe7f18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_F =file_io.FileIO(\"gs://bucket-git-m2i/DB/golden/merging_F.csv\",mode=\"w\")    \n",
    "merged_F.write(\"Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\\n\")  \n",
    "for num in range(1,3):\n",
    "    ff = file_io.FileIO(\"gs://bucket-git-m2i/DB/historique/F\"+str(num)+\".csv\",mode=\"r\")\n",
    "    next(ff) # skip the header\n",
    "    for line in ff:\n",
    "        merged_F.write(line)\n",
    "    ff.close() # not really needed\n",
    "merged_F.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc5cf5-249b-481d-a3d4-e085d076c2d1",
   "metadata": {},
   "source": [
    "### Le csv créé (merging_F) est importé dans un dataset bigQuery (en écrasant les donnnées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb74f9c4-8937-45a4-83d5-7c0a088ccf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2208 rows.\n"
     ]
    }
   ],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"micro-atrium-360309.m2i_pronostics.merging-F\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    autodetect=True,\n",
    "    # The source format defaults to CSV, so the line below is optional.\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    ")\n",
    "uri = \"gs://bucket-git-m2i/DB/golden/merging_F.csv\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)  # Make an API request.\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80763e48-7f9c-4db4-8031-1ec262841881",
   "metadata": {},
   "source": [
    "## Se basant sur merging historique -> calcul (dans variables) des statistiques de la ligue (toutes équipes confondues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "936add1c-5b19-4079-a084-c6e4b5f70a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://bucket-git-m2i/DB/golden/merging_F.csv', sep=',')\n",
    "### normalizer tous les noms des equipes dans le dataframe\n",
    "# Home \n",
    "df['HomeTeam'] = df['HomeTeam'].str.strip().str.upper()\n",
    "df['HomeTeam'] = df['HomeTeam'].apply(nom_normalizer)\n",
    "# Away\n",
    "df['AwayTeam'] = df['AwayTeam'].str.strip().str.upper()\n",
    "df['AwayTeam'] = df['AwayTeam'].apply(nom_normalizer)\n",
    "\n",
    "# print(df)\n",
    "\n",
    "home_total = df[['FTHG']].sum()[0]\n",
    "home_average = df[['FTHG']].mean(numeric_only=True)[0]\n",
    "nb_of_matches = df[['HomeTeam']].count()[0]\n",
    "# print(home_total)\n",
    "# print(home_average)\n",
    "# print(nb_of_matches)\n",
    "\n",
    "away_total = df[['FTAG']].sum()[0]\n",
    "away_average = df[['FTAG']].mean(numeric_only=True)[0]\n",
    "# print(away_total)\n",
    "# print(away_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d61e82-a0e0-4132-b236-d6fc2363b9e3",
   "metadata": {},
   "source": [
    "## Calcul d'un scoring de chaque équipe (indépendemment de son adversaire) selon away/home relatif aux buts encaissés et marqués -> création csv reprenant ces résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdb38a0a-5f75-47e2-b7ba-e46f7161802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_io.FileIO('gs://bucket-git-m2i/DB/golden/merging_calendrier.csv',mode=\"r\") as calendrier:\n",
    "    csv_reader = csv.reader(calendrier)\n",
    "    with file_io.FileIO('gs://bucket-git-m2i/DB/golden/final-results.csv', mode='w') as new_file:\n",
    "        # on ouvre un csv pour sauvegarder les resultats\n",
    "        csv_writer = csv.writer(new_file)                \n",
    "\n",
    "        for line in calendrier:# line est un chaine des caracs\n",
    "            data_line = line.split(sep=',') # separer les donnée\n",
    "            domicile = nom_normalizer(data_line[1].strip().upper()) # recuperer l'equipe domicile\n",
    "            visiteur = nom_normalizer(data_line[2].strip().upper()) # recuperer l'equipe visiteur\n",
    "            \n",
    "            \n",
    "            team_home_df = df[df['HomeTeam']==domicile]\n",
    "            team_home_total = team_home_df[['FTHG']].sum()[0]\n",
    "            team_home_average = team_home_df[['FTHG']].mean()[0]\n",
    "            home_conceed_total = team_home_df[['FTAG']].sum()[0]\n",
    "            home_conceed_average = team_home_df[['FTAG']].mean()[0]\n",
    "            nb_matches_home = team_home_df[['FTAG']].count()[0]\n",
    "            home_attack_str = (team_home_average / home_average)\n",
    "            home_defence_str = (home_conceed_average / away_average)\n",
    "\n",
    "            team_away_df = df[df['AwayTeam']==visiteur]\n",
    "            away_score_total = team_away_df[['FTAG']].sum()[0]\n",
    "            away_score_average = team_away_df[['FTAG']].mean()[0]\n",
    "            away_conceed_total = team_away_df[['FTHG']].sum()[0]\n",
    "            away_conceed_average = team_away_df[['FTHG']].mean()[0]\n",
    "            nb_matches_away = team_away_df[['FTHG']].count()[0]\n",
    "            away_attack_str = (away_score_average / away_average)\n",
    "            away_defence_str = (away_conceed_average / home_average)\n",
    "\n",
    "            #csv_writer.writerow(str(away_attack_str)+','+str(away_defence_str)+'\\n')\n",
    "            new_file.write(str(domicile)+','+str(visiteur)+','+str(home_attack_str).strip()+','+str(home_defence_str).strip()+','+str(away_attack_str).strip()+','+str(away_defence_str).strip()+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d47984-9b90-4f91-865a-28f661c329c7",
   "metadata": {},
   "source": [
    "## Calcul des probabilités du résultat (draw / home_win / away_win) -> création csv reprenant ces résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c4cbe73-4862-4b72-94ac-1c6cd70cafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_io.FileIO('gs://bucket-git-m2i/DB/golden/final-results.csv', mode='r') as matchs:\n",
    "    csv_reader = csv.reader(matchs)\n",
    "    with file_io.FileIO('gs://bucket-git-m2i/DB/golden/match-prob.csv', mode='w') as probs:\n",
    "        # on ouvre un csv pour sauvegarder les resultats\n",
    "        csv_writer = csv.writer(probs) \n",
    "        for line in matchs: # line est un chaine des caracs\n",
    "            data_line = line.split(sep=',') # separer les donnée\n",
    "            domicile = nom_normalizer(data_line[0].strip().upper()) # recuperer l'equipe domicile\n",
    "            visiteur = nom_normalizer(data_line[1].strip().upper()) # recuperer l'equipe visiteur\n",
    "            away_attack_str = data_line[2].strip()\n",
    "            away_defence_str = data_line[3].strip()\n",
    "            home_defence_str = data_line[4].strip()\n",
    "            home_attack_str = data_line[5].strip()\n",
    "\n",
    "            home_expect = float(home_attack_str) * float(away_defence_str) * float(home_average)\n",
    "            away_expect = float(away_attack_str) * float(home_defence_str) * float(away_average)\n",
    "            \n",
    "            home_goals_prob = []\n",
    "            for i in range(8):\n",
    "                expect = poisson_probability(home_expect, i)\n",
    "                home_goals_prob.append(expect)\n",
    "            home_goals_probs = np.round(home_goals_prob,2)\n",
    "            away_goals_prob = []\n",
    "            for i in range(8):\n",
    "                expect = poisson_probability(away_expect, i)\n",
    "                away_goals_prob.append(expect)  \n",
    "            away_goals_probs = np.round(away_goals_prob,2) \n",
    "            p = {'Home0':[((home_goals_probs[0]*away_goals_probs[0])/100), ((home_goals_probs[0]*away_goals_probs[1])/100), ((home_goals_probs[0]*away_goals_probs[2])/100), ((home_goals_probs[0]*away_goals_probs[3])/100), ((home_goals_probs[0]*away_goals_probs[4])/100), ((home_goals_probs[0]*away_goals_probs[5])/100),((home_goals_probs[0]*away_goals_probs[6])/100),((home_goals_probs[0]*away_goals_probs[7])/100)], \n",
    "                'Home1':[((home_goals_probs[1]*away_goals_probs[0])/100), ((home_goals_probs[1]*away_goals_probs[1])/100), ((home_goals_probs[1]*away_goals_probs[2])/100), ((home_goals_probs[1]*away_goals_probs[3])/100), ((home_goals_probs[1]*away_goals_probs[4])/100), ((home_goals_probs[1]*away_goals_probs[5])/100),((home_goals_probs[1]*away_goals_probs[6])/100),((home_goals_probs[1]*away_goals_probs[7])/100)], \n",
    "                'Home2':[((home_goals_probs[2]*away_goals_probs[0])/100), ((home_goals_probs[2]*away_goals_probs[1])/100), ((home_goals_probs[2]*away_goals_probs[2])/100), ((home_goals_probs[2]*away_goals_probs[3])/100), ((home_goals_probs[2]*away_goals_probs[4])/100), ((home_goals_probs[2]*away_goals_probs[5])/100),((home_goals_probs[2]*away_goals_probs[6])/100),((home_goals_probs[2]*away_goals_probs[7])/100)], \n",
    "                'Home3':[((home_goals_probs[3]*away_goals_probs[0])/100), ((home_goals_probs[3]*away_goals_probs[1])/100), ((home_goals_probs[3]*away_goals_probs[2])/100), ((home_goals_probs[3]*away_goals_probs[3])/100), ((home_goals_probs[3]*away_goals_probs[4])/100), ((home_goals_probs[3]*away_goals_probs[5])/100),((home_goals_probs[3]*away_goals_probs[6])/100),((home_goals_probs[3]*away_goals_probs[7])/100)], \n",
    "                'Home4':[((home_goals_probs[4]*away_goals_probs[0])/100), ((home_goals_probs[4]*away_goals_probs[1])/100), ((home_goals_probs[4]*away_goals_probs[2])/100), ((home_goals_probs[4]*away_goals_probs[3])/100), ((home_goals_probs[4]*away_goals_probs[4])/100), ((home_goals_probs[4]*away_goals_probs[5])/100),((home_goals_probs[4]*away_goals_probs[6])/100),((home_goals_probs[4]*away_goals_probs[7])/100)], \n",
    "                'Home5':[((home_goals_probs[5]*away_goals_probs[0])/100), ((home_goals_probs[5]*away_goals_probs[1])/100), ((home_goals_probs[5]*away_goals_probs[2])/100), ((home_goals_probs[5]*away_goals_probs[3])/100), ((home_goals_probs[5]*away_goals_probs[4])/100), ((home_goals_probs[5]*away_goals_probs[5])/100),((home_goals_probs[5]*away_goals_probs[6])/100),((home_goals_probs[5]*away_goals_probs[7])/100)],\n",
    "                'Home6':[((home_goals_probs[6]*away_goals_probs[0])/100), ((home_goals_probs[6]*away_goals_probs[1])/100), ((home_goals_probs[6]*away_goals_probs[2])/100), ((home_goals_probs[6]*away_goals_probs[3])/100), ((home_goals_probs[6]*away_goals_probs[4])/100), ((home_goals_probs[6]*away_goals_probs[5])/100),((home_goals_probs[6]*away_goals_probs[6])/100),((home_goals_probs[6]*away_goals_probs[7])/100)],\n",
    "                'Home7':[((home_goals_probs[7]*away_goals_probs[0])/100), ((home_goals_probs[7]*away_goals_probs[1])/100), ((home_goals_probs[7]*away_goals_probs[2])/100), ((home_goals_probs[7]*away_goals_probs[3])/100), ((home_goals_probs[7]*away_goals_probs[4])/100), ((home_goals_probs[7]*away_goals_probs[5])/100),((home_goals_probs[7]*away_goals_probs[6])/100),((home_goals_probs[7]*away_goals_probs[7])/100)]}\n",
    "            probability = pd.DataFrame(p, index=['away0','away1','away2', 'away3', 'away4', 'away5', 'away6', 'away7'])\n",
    "            nump = probability.to_numpy()\n",
    "            draw_prob = np.trace(nump)\n",
    "            Home_win_prob = (np.trace(nump, offset = 1))+(np.trace(nump, offset = 2))+(np.trace(nump, offset = 3))+(np.trace(nump, offset = 4))+(np.trace(nump, offset = 5))+(np.trace(nump, offset = 6))\n",
    "            away_win_prob = (np.trace(nump, offset = -1))+(np.trace(nump, offset = -2))+(np.trace(nump, offset = -3))+(np.trace(nump, offset = -4))+(np.trace(nump, offset = -5))+(np.trace(nump, offset = -6))\n",
    "            probs.write(domicile+','+visiteur+','+str(draw_prob).strip()+','+str(Home_win_prob)+','+str(away_win_prob)+'\\n')\n",
    "matchs.close()\n",
    "probs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2ceca-1bb2-4be9-a863-dba3c1a9bbf5",
   "metadata": {},
   "source": [
    "### Le csv créé (match-prob) est importé dans un dataset bigQuery (en écrasant les donnnées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d95783a-7bb0-4596-b38c-c49e61109d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 370 rows.\n"
     ]
    }
   ],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"micro-atrium-360309.m2i_pronostics.match-prob\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    # The source format defaults to CSV, so the line below is optional.\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    ")\n",
    "uri = \"gs://bucket-git-m2i/DB/golden/match-prob.csv\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)  # Make an API request.\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003b2eb-2b18-4b7e-8116-0692de637ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
